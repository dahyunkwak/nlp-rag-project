{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2456622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# LangChain & Upstage\n",
    "from langchain_upstage import UpstageEmbeddings, ChatUpstage\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Wikipedia (ì§€ì •ëœ APIë§Œ ì‚¬ìš©)\n",
    "import wikipediaapi\n",
    "\n",
    "# API Key ì„¤ì •\n",
    "os.environ.setdefault(\"UPSTAGE_API_KEY\", \"up_g01lLro3nefoqy9IZGw3NK4ExjfIQ\") # ë³¸ì¸ì˜ í‚¤ë¡œ ëŒ€ì²´í•˜ì„¸ìš”\n",
    "UPSTAGE_API_KEY = os.environ.get(\"UPSTAGE_API_KEY\", \"\")\n",
    "\n",
    "# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "CURRENT_DIR = Path(__file__).parent if \"__file__\" in globals() else Path.cwd()\n",
    "if Path.cwd() != CURRENT_DIR:\n",
    "    os.chdir(CURRENT_DIR)\n",
    "\n",
    "print(f\"âœ… ì‘ì—… ë””ë ‰í† ë¦¬: {CURRENT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47c59ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# 1. ì´í™”ì—¬ëŒ€ í•™ì¹™ ë°ì´í„° ë¡œë”© (PDF & CSV)\n",
    "# ==================================================================\n",
    "\n",
    "def split_by_articles(text: str, is_appendix: bool = False) -> List[str]:\n",
    "    \"\"\"\n",
    "    PDF í…ìŠ¤íŠ¸ë¥¼ ì¡°í•­ë³„ë¡œ ì •í™•íˆ ë¶„ë¦¬í•˜ëŠ” í•¨ìˆ˜\n",
    "    ì´í™”ì—¬ëŒ€ í•™ì¹™ êµ¬ì¡° ë¶„ì„:\n",
    "    - ë³„í‘œ 1, ë³„í‘œ 2, ë³„í‘œ 3 ë“± (í‘œ)\n",
    "    - ì œ1ì¡°, ì œ2ì¡°, ì œ3ì¡° ë“± (ì¡°í•­)\n",
    "    - â‘ , â‘¡, â‘¢ ë“± (í•­ëª©)\n",
    "    - ë¶€ì¹™ (ë¶€ì¹™ ì‹œì‘)\n",
    "    \"\"\"\n",
    "    # ì¡°í•­ ì‹œì‘ íŒ¨í„´ (ìš°ì„ ìˆœìœ„ ìˆœì„œëŒ€ë¡œ)\n",
    "    # 1. ë³„í‘œ (í‘œëŠ” ë…ë¦½ì ì¸ ë‹¨ìœ„)\n",
    "    # 2. ì œXì¡° (ì¡°í•­)\n",
    "    # 3. â‘ í•­, â‘¡í•­ ë“± (í•­ëª© - ì¡°í•­ ë‚´ ì„¸ë¶€ì‚¬í•­)\n",
    "    \n",
    "    splits = []\n",
    "    \n",
    "    # íŒ¨í„´ 1: ë³„í‘œë¡œ ì‹œì‘í•˜ëŠ” ë¶€ë¶„ (ê°€ì¥ í° ë‹¨ìœ„)\n",
    "    star_pattern = r'(ë³„í‘œ\\s*\\d+)'\n",
    "    star_matches = list(re.finditer(star_pattern, text))\n",
    "    \n",
    "    # íŒ¨í„´ 2: ì œXì¡°ë¡œ ì‹œì‘í•˜ëŠ” ë¶€ë¶„\n",
    "    article_pattern = r'(ì œ\\d+ì¡°(?:\\s*\\([^)]+\\))?)'  # ì œ1ì¡°, ì œ1ì¡°(ëª©ì ) ë“±\n",
    "    article_matches = list(re.finditer(article_pattern, text))\n",
    "    \n",
    "    # ë³„í‘œê°€ ìˆìœ¼ë©´ ë³„í‘œ ê¸°ì¤€ìœ¼ë¡œ ë¨¼ì € ë¶„ë¦¬\n",
    "    if star_matches:\n",
    "        last_pos = 0\n",
    "        for match in star_matches:\n",
    "            pos = match.start()\n",
    "            if pos > last_pos:\n",
    "                chunk = text[last_pos:pos].strip()\n",
    "                if chunk:\n",
    "                    splits.append(chunk)\n",
    "            last_pos = pos\n",
    "        # ë§ˆì§€ë§‰ ë³„í‘œ ì´í›„ í…ìŠ¤íŠ¸\n",
    "        if last_pos < len(text):\n",
    "            chunk = text[last_pos:].strip()\n",
    "            if chunk:\n",
    "                splits.append(chunk)\n",
    "        \n",
    "        # ë³„í‘œë¡œ ë¶„ë¦¬ëœ ê° ì²­í¬ë¥¼ ì œXì¡°ë¡œ ë‹¤ì‹œ ë¶„ë¦¬\n",
    "        refined_splits = []\n",
    "        for split in splits:\n",
    "            split_article_matches = list(re.finditer(article_pattern, split))\n",
    "            if split_article_matches:\n",
    "                split_last_pos = 0\n",
    "                for match in split_article_matches:\n",
    "                    pos = match.start()\n",
    "                    if pos > split_last_pos:\n",
    "                        chunk = split[split_last_pos:pos].strip()\n",
    "                        if chunk:\n",
    "                            refined_splits.append(chunk)\n",
    "                    split_last_pos = pos\n",
    "                if split_last_pos < len(split):\n",
    "                    chunk = split[split_last_pos:].strip()\n",
    "                    if chunk:\n",
    "                        refined_splits.append(chunk)\n",
    "            else:\n",
    "                refined_splits.append(split)\n",
    "        splits = refined_splits\n",
    "    elif article_matches:\n",
    "        # ë³„í‘œê°€ ì—†ìœ¼ë©´ ì œXì¡° ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬\n",
    "        last_pos = 0\n",
    "        for match in article_matches:\n",
    "            pos = match.start()\n",
    "            if pos > last_pos:\n",
    "                chunk = text[last_pos:pos].strip()\n",
    "                if chunk:\n",
    "                    splits.append(chunk)\n",
    "            last_pos = pos\n",
    "        # ë§ˆì§€ë§‰ ì¡°í•­ ì´í›„ í…ìŠ¤íŠ¸\n",
    "        if last_pos < len(text):\n",
    "            chunk = text[last_pos:].strip()\n",
    "            if chunk:\n",
    "                splits.append(chunk)\n",
    "    else:\n",
    "        # ë³„í‘œë„ ì œXì¡°ë„ ì—†ìœ¼ë©´ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ\n",
    "        splits = [text]\n",
    "    \n",
    "    # ë¶€ì¹™ì˜ ê²½ìš° \"ë¶€ì¹™\" í‚¤ì›Œë“œë¡œë„ ë¶„ë¦¬\n",
    "    if is_appendix and splits:\n",
    "        final_splits = []\n",
    "        for split in splits:\n",
    "            # \"ë¶€ì¹™\" í‚¤ì›Œë“œê°€ ì¤‘ê°„ì— ë‚˜ì˜¤ë©´ ë¶„ë¦¬\n",
    "            if \"ë¶€ì¹™\" in split:\n",
    "                parts = re.split(r'(ë¶€ì¹™)', split, maxsplit=1)\n",
    "                if len(parts) > 1:\n",
    "                    if parts[0].strip():\n",
    "                        final_splits.append(parts[0].strip())\n",
    "                    if len(parts) > 2:\n",
    "                        final_splits.append(parts[1] + parts[2].strip())\n",
    "                else:\n",
    "                    final_splits.append(split)\n",
    "            else:\n",
    "                final_splits.append(split)\n",
    "        splits = final_splits\n",
    "    \n",
    "    # ë¹ˆ ì²­í¬ ì œê±° ë° ìµœì†Œ ê¸¸ì´ ì²´í¬\n",
    "    splits = [s for s in splits if s and len(s.strip()) > 50]\n",
    "    \n",
    "    return splits if splits else [text]\n",
    "\n",
    "# ==================================================================\n",
    "# 1-1. PDF ë¡œë”© ë° ì²­í‚¹\n",
    "# ==================================================================\n",
    "PDF_PATH = CURRENT_DIR / \"ewha\" / \"ewha.pdf\"\n",
    "if not PDF_PATH.exists():\n",
    "    PDF_PATH = CURRENT_DIR / \"ewha.pdf\"\n",
    "\n",
    "ewha_documents = []\n",
    "\n",
    "if PDF_PATH.exists():\n",
    "    print(\"ğŸ“„ ì´í™”ì—¬ëŒ€ í•™ì¹™ PDF ë¡œë”© ì¤‘...\")\n",
    "    loader = PyPDFLoader(str(PDF_PATH))\n",
    "    docs = loader.load()\n",
    "    docs = [d for d in docs if int(d.metadata.get(\"page\", 0)) < 38] # ë¶€ì¹™ ì „ê¹Œì§€\n",
    "    \n",
    "    full_text = \"\\n\".join([d.page_content for d in docs])\n",
    "    split_index = full_text.find(\"ë¶€ì¹™\")\n",
    "    \n",
    "    main_text = full_text[:split_index] if split_index != -1 else full_text\n",
    "    appendix_text = full_text[split_index:] if split_index != -1 else \"\"\n",
    "    \n",
    "    print(f\"ğŸ“Œ ë¶€ì¹™ ì‹œì‘ ì¸ë±ìŠ¤: {split_index if split_index != -1 else 'ì—†ìŒ'}\")\n",
    "    print(f\"ğŸ“„ ë³¸ë¬¸ ê¸¸ì´: {len(main_text)}\")\n",
    "    print(f\"ğŸ“„ ë¶€ì¹™ ê¸¸ì´: {len(appendix_text)}\")\n",
    "    \n",
    "    # ë³¸ë¬¸ì„ ì¡°í•­ë³„ë¡œ ë¶„ë¦¬\n",
    "    print(\"ğŸ“‹ ë³¸ë¬¸ ì¡°í•­ë³„ ë¶„ë¦¬ ì¤‘...\")\n",
    "    main_articles = split_by_articles(main_text, is_appendix=False)\n",
    "    print(f\"   â†’ {len(main_articles)}ê°œ ì¡°í•­ìœ¼ë¡œ ë¶„ë¦¬ë¨\")\n",
    "    \n",
    "    # ê° ì¡°í•­ì„ ì²­í‚¹ (ì¡°í•­ì´ ë„ˆë¬´ ê¸¸ ê²½ìš°ë§Œ)\n",
    "    main_chunks = []\n",
    "    for i, article in enumerate(main_articles):\n",
    "        # ì¡°í•­ì´ 1200ì ì´í•˜ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "        if len(article) <= 1200:\n",
    "            main_chunks.append(Document(\n",
    "                page_content=article,\n",
    "                metadata={\"source\": \"ewha.pdf ë³¸ë¬¸\", \"type\": \"main_text\", \"article_index\": i}\n",
    "            ))\n",
    "        else:\n",
    "            # ì¡°í•­ì´ ê¸¸ë©´ ì„¸ë¶€ í•­ëª©ë³„ë¡œ ë¶„ë¦¬\n",
    "            text_splitter_main = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1200,\n",
    "                chunk_overlap=200,  # overlap ê°ì†Œ (ì¡°í•­ ê²½ê³„ ìœ ì§€)\n",
    "                separators=[\"\\n\\nâ‘ \", \"\\n\\nâ‘¡\", \"\\n\\nâ‘¢\", \"\\n\\nâ‘£\", \"\\n\\nâ‘¤\", \"\\n\\nâ‘¥\", \"\\n\\nâ‘¦\", \"\\n\\nâ‘§\", \"\\n\\nâ‘¨\", \"\\n\\nâ‘©\",\n",
    "                           \"\\nâ‘ \", \"\\nâ‘¡\", \"\\nâ‘¢\", \"\\nâ‘£\", \"\\nâ‘¤\", \"\\nâ‘¥\", \"\\nâ‘¦\", \"\\nâ‘§\", \"\\nâ‘¨\", \"\\nâ‘©\",\n",
    "                           \"\\n\\nì œ\", \"\\nì œ\", \"\\n\\n\", \"\\n\", \". \", \" \"],\n",
    "                length_function=len\n",
    "            )\n",
    "            sub_chunks = text_splitter_main.split_text(article)\n",
    "            for j, chunk in enumerate(sub_chunks):\n",
    "                # ê° ì²­í¬ì˜ ê¸¸ì´ í™•ì¸ (ì•ˆì „ì¥ì¹˜)\n",
    "                if len(chunk) > 3000:  # 3000ì ì´ˆê³¼ ì‹œ ê²½ê³ \n",
    "                    print(f\"âš ï¸ ê²½ê³ : ë³¸ë¬¸ ì¡°í•­ {i}ì˜ ì„¸ë¶€ ì²­í¬ {j}ê°€ 3000ìë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤ ({len(chunk)}ì).\")\n",
    "                main_chunks.append(Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\"source\": \"ewha.pdf ë³¸ë¬¸\", \"type\": \"main_text\", \"article_index\": i, \"sub_index\": j}\n",
    "                ))\n",
    "    \n",
    "    print(f\"âœ… ë³¸ë¬¸ ì²­í¬ ìˆ˜: {len(main_chunks)}ê°œ\")\n",
    "    ewha_documents.extend(main_chunks)\n",
    "    \n",
    "    # ë¶€ì¹™ì„ ì¡°í•­ë³„ë¡œ ë¶„ë¦¬\n",
    "    if appendix_text:\n",
    "        print(\"ğŸ“‹ ë¶€ì¹™ ì¡°í•­ë³„ ë¶„ë¦¬ ì¤‘...\")\n",
    "        appendix_articles = split_by_articles(appendix_text, is_appendix=True)\n",
    "        print(f\"   â†’ {len(appendix_articles)}ê°œ ì¡°í•­ìœ¼ë¡œ ë¶„ë¦¬ë¨\")\n",
    "        \n",
    "        appendix_chunks = []\n",
    "        for i, article in enumerate(appendix_articles):\n",
    "            # ë¶€ì¹™ ì¡°í•­ì€ ë” ì‘ê²Œ ì²­í‚¹ (ì„¸ë¶€ ê·œì •ì´ ë§ìŒ)\n",
    "            if len(article) <= 1000:\n",
    "                appendix_chunks.append(Document(\n",
    "                    page_content=article,\n",
    "                    metadata={\"source\": \"ewha.pdf ë¶€ì¹™\", \"type\": \"appendix_text\", \"article_index\": i}\n",
    "                ))\n",
    "            else:\n",
    "                # ë¶€ì¹™ ì¡°í•­ì´ ê¸¸ë©´ ì„¸ë¶€ í•­ëª©ë³„ë¡œ ë¶„ë¦¬\n",
    "                text_splitter_appendix = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=1000,\n",
    "                    chunk_overlap=150,  # overlap ê°ì†Œ\n",
    "                    separators=[\"\\n\\nâ‘ \", \"\\n\\nâ‘¡\", \"\\n\\nâ‘¢\", \"\\n\\nâ‘£\", \"\\n\\nâ‘¤\", \"\\n\\nâ‘¥\", \"\\n\\nâ‘¦\", \"\\n\\nâ‘§\", \"\\n\\nâ‘¨\", \"\\n\\nâ‘©\",\n",
    "                               \"\\nâ‘ \", \"\\nâ‘¡\", \"\\nâ‘¢\", \"\\nâ‘£\", \"\\nâ‘¤\", \"\\nâ‘¥\", \"\\nâ‘¦\", \"\\nâ‘§\", \"\\nâ‘¨\", \"\\nâ‘©\",\n",
    "                               \"ë³„í‘œ\", \"\\n\\nì œ\", \"\\nì œ\", \"\\n\\n\", \"\\n\", \". \", \" \"],\n",
    "                    length_function=len\n",
    "                )\n",
    "                sub_chunks = text_splitter_appendix.split_text(article)\n",
    "                for j, chunk in enumerate(sub_chunks):\n",
    "                    # ê° ì²­í¬ì˜ ê¸¸ì´ í™•ì¸ (ì•ˆì „ì¥ì¹˜)\n",
    "                    if len(chunk) > 3000:  # 3000ì ì´ˆê³¼ ì‹œ ê²½ê³ \n",
    "                        print(f\"âš ï¸ ê²½ê³ : ë¶€ì¹™ ì¡°í•­ {i}ì˜ ì„¸ë¶€ ì²­í¬ {j}ê°€ 3000ìë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤ ({len(chunk)}ì).\")\n",
    "                    appendix_chunks.append(Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={\"source\": \"ewha.pdf ë¶€ì¹™\", \"type\": \"appendix_text\", \"article_index\": i, \"sub_index\": j}\n",
    "                    ))\n",
    "        \n",
    "        print(f\"âœ… ë¶€ì¹™ ì²­í¬ ìˆ˜: {len(appendix_chunks)}ê°œ\")\n",
    "        ewha_documents.extend(appendix_chunks)\n",
    "    else:\n",
    "        appendix_chunks = []\n",
    "    \n",
    "    print(f\"\\nğŸ“¦ ì´ PDF í…ìŠ¤íŠ¸ ì²­í¬: {len(main_chunks) + len(appendix_chunks) if appendix_text else len(main_chunks)}ê°œ\")\n",
    "    print(f\"   - ë³¸ë¬¸: {len(main_chunks)}ê°œ\")\n",
    "    if appendix_text:\n",
    "        print(f\"   - ë¶€ì¹™: {len(appendix_chunks)}ê°œ\")\n",
    "    print(f\"   - ìµœëŒ€ ì²­í¬ ê¸¸ì´: {max(len(d.page_content) for d in ewha_documents) if ewha_documents else 0}ì\")\n",
    "else:\n",
    "    print(\"âš ï¸ ê²½ê³ : ewha.pdf íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ==================================================================\n",
    "# 1-2. CSV í‘œ ë°ì´í„° ì²˜ë¦¬ ë° ë¬¸ì¥ ìƒì„±\n",
    "# ==================================================================\n",
    "\n",
    "def coalesce(*values: object) -> str:\n",
    "    \"\"\"ì²« ë²ˆì§¸ ìœ íš¨í•œ ê°’ì„ ë°˜í™˜\"\"\"\n",
    "    for value in values:\n",
    "        if value is None:\n",
    "            continue\n",
    "        if isinstance(value, float) and pd.isna(value):\n",
    "            continue\n",
    "        text = str(value).strip()\n",
    "        if text:\n",
    "            return text\n",
    "    return \"\"\n",
    "\n",
    "def build_degree_sentences_from_csv(df: pd.DataFrame, year: str = \"ìµœì‹  ê°œì •\") -> List[str]:\n",
    "    \"\"\"CSVì—ì„œ í•™ìœ„ ë¬¸ì¥ ìƒì„±\"\"\"\n",
    "    sentences = []\n",
    "    for idx, row in df.iterrows():\n",
    "        college = coalesce(row.get(\"ì„¤ì¹˜ëŒ€í•™\"), row.get(\"ëŒ€í•™\"))\n",
    "        degree = coalesce(row.get(\"í•™ìœ„_ì¢…ë¥˜\"), row.get(\"í•™ìœ„ì¢…ë¥˜\"), row.get(\"í•™ì‚¬\"))\n",
    "        major = coalesce(row.get(\"í•™ê³¼_ì „ê³µ\"), row.get(\"í•™ê³¼ ë˜ëŠ” ì „ê³µ\"), row.get(\"ì „ê³µ\"))\n",
    "        \n",
    "        if not college or not degree or not major:\n",
    "            continue\n",
    "            \n",
    "        sentence = f\"{college}ì˜ {major} ì „ê³µì€ {degree} í•™ìœ„ë¥¼ ìˆ˜ì—¬í•œë‹¤. ({year})\"\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "def build_quota_text_v2(row: pd.Series) -> str:\n",
    "    \"\"\"capacity.csv -> ì…í•™ì •ì› ë¬¸ì¥ ìƒì„± (ì™„ì „íŒ)\"\"\"\n",
    "    year = coalesce(row.get(\"year\"))\n",
    "    college = coalesce(row.get(\"college\"))\n",
    "    department = coalesce(row.get(\"department\"))\n",
    "    major = coalesce(row.get(\"major\"))\n",
    "    quota = coalesce(row.get(\"quota\"))\n",
    "\n",
    "    # ì—°ë„ ì²˜ë¦¬\n",
    "    year_str = f\"{int(year)}í•™ë…„ë„\" if str(year).isdigit() else year\n",
    "\n",
    "    # quota ì •ë¦¬\n",
    "    if not quota or str(quota).strip().lower() == \"null\":\n",
    "        quota = \"ë¯¸ìƒ\"\n",
    "    else:\n",
    "        quota = re.sub(r'\\(.*?\\)|\\[.*?\\]', '', str(quota)).strip()\n",
    "\n",
    "    # ì¼€ì´ìŠ¤ë³„ ë¬¸ì¥ ìƒì„± --------------------------------------------\n",
    "\n",
    "    # 1) majorê°€ ìˆê³  quotaê°€ Nullì´ë©´ â†’ ì „ê³µë³„ ì •ì› ë¯¸ìƒ ì¼€ì´ìŠ¤\n",
    "    if major and quota == \"ë¯¸ìƒ\":\n",
    "        return f\"{year_str} {college} {department} ì†Œì† {major}ì˜ ì…í•™ì •ì›ì€ ë¯¸ìƒì…ë‹ˆë‹¤.\"\n",
    "\n",
    "    # 2) majorê°€ ìˆê³  quota ìˆ«ì ì¡´ì¬ â†’ ì „ê³µ ë‹¨ìœ„ ì •ì›\n",
    "    if major and quota != \"ë¯¸ìƒ\":\n",
    "        return f\"{year_str} {college} {department} ì†Œì† {major}ì˜ ì…í•™ì •ì›ì€ {quota}ëª…ì…ë‹ˆë‹¤.\"\n",
    "\n",
    "    # 3) major ì—†ê³  department ìˆê³  quota ìˆìŒ â†’ í•™ë¶€/í•™ê³¼ ì „ì²´ ì •ì›\n",
    "    if department and quota != \"ë¯¸ìƒ\":\n",
    "        return f\"{year_str} {college} {department}ì˜ ì…í•™ì •ì›ì€ {quota}ëª…ì…ë‹ˆë‹¤.\"\n",
    "\n",
    "    # 4) college ì „ì²´ ì •ì›\n",
    "    if \"ì „ì²´\" in str(department):\n",
    "        return f\"{year_str} {college} ì „ì²´ ì…í•™ì •ì›ì€ {quota}ëª…ì…ë‹ˆë‹¤.\"\n",
    "\n",
    "    # 5) fallback\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "\n",
    "def build_contract_text(row: pd.Series) -> str:\n",
    "    \"\"\"ê³„ì•½í•™ê³¼ í…ìŠ¤íŠ¸ ìƒì„±\"\"\"\n",
    "    college = coalesce(row.get(\"ì„¤ì¹˜ëŒ€í•™\"))\n",
    "    form = coalesce(row.get(\"ì„¤ì¹˜í˜•íƒœ\"))\n",
    "    major = coalesce(row.get(\"í•™ê³¼_ì „ê³µ\"))\n",
    "    degree = coalesce(row.get(\"í•™ìœ„_ì¢…ë¥˜\"))\n",
    "    quota = coalesce(row.get(\"ì…í•™ì •ì›_ëª…\"))\n",
    "    period = coalesce(row.get(\"ì„¤ì¹˜_ìš´ì˜ê¸°ê°„\"))\n",
    "    \n",
    "    text = (\n",
    "        f\"ê³„ì•½í•™ê³¼ ì„¤ì¹˜Â·ìš´ì˜ ì •ë³´ì…ë‹ˆë‹¤: ì„¤ì¹˜ëŒ€í•™ì€ {college}, ì„¤ì¹˜í˜•íƒœëŠ” {form}, \"\n",
    "        f\"í•™ê³¼/ì „ê³µì€ {major}, ìˆ˜ì—¬ í•™ìœ„ëŠ” {degree}ì´ë©°, \"\n",
    "        f\"ì…í•™ ì •ì›ì€ {quota}ëª…ì…ë‹ˆë‹¤. ì„¤ì¹˜Â·ìš´ì˜ ê¸°ê°„ì€ {period}ì…ë‹ˆë‹¤.\"\n",
    "    )\n",
    "    return text\n",
    "\n",
    "def build_grade_text(row: pd.Series) -> str:\n",
    "    \"\"\"ì„±ì ì  í…ìŠ¤íŠ¸ ìƒì„±\"\"\"\n",
    "    year = coalesce(row.get(\"year\"))\n",
    "    grade = coalesce(row.get(\"grade\"))\n",
    "    gpa = coalesce(row.get(\"gpa\"))\n",
    "    \n",
    "    if not year or not grade or pd.isna(gpa):\n",
    "        return \"\"\n",
    "    \n",
    "    # yearê°€ \"from 1980\"ì´ë©´ \"1980í•™ë…„ë„ ì´í›„\", \"before 1980\"ì´ë©´ \"1980í•™ë…„ë„ ì´ì „\"\n",
    "    if \"from 1980\" in str(year).lower() or \"1980 ì´í›„\" in str(year):\n",
    "        year_desc = \"1980í•™ë…„ë„ ì´í›„ ì…í•™ìƒ\"\n",
    "    elif \"before 1980\" in str(year).lower() or \"1980 ì´ì „\" in str(year):\n",
    "        year_desc = \"1980í•™ë…„ë„ ì´ì „ ì…í•™ìƒ\"\n",
    "    else:\n",
    "        year_desc = str(year)\n",
    "    \n",
    "    return f\"{year_desc}ì— ì ìš©í•˜ëŠ” ë“±ê¸‰ {grade}ì˜ ì„±ì ì ì€ {gpa}ì ì…ë‹ˆë‹¤.\"\n",
    "\n",
    "# CSV íŒŒì¼ ì²˜ë¦¬\n",
    "csv_documents = []\n",
    "\n",
    "# CSV íŒŒì¼ ê²½ë¡œ ì„¤ì • (ewha csvs í´ë” ìš°ì„ )\n",
    "csvs_dir = CURRENT_DIR / \"ewha\" / \"ewha csvs\"\n",
    "if not csvs_dir.exists():\n",
    "    csvs_dir = CURRENT_DIR / \"ewha csvs\"  # í•˜ìœ„ í˜¸í™˜ì„±: í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ìˆì„ ê²½ìš°\n",
    "if not csvs_dir.exists():\n",
    "    csvs_dir = CURRENT_DIR  # í´ë”ê°€ ì—†ìœ¼ë©´ í˜„ì¬ ë””ë ‰í† ë¦¬ ì‚¬ìš©\n",
    "\n",
    "# 1) degrees ê´€ë ¨ íŒŒì¼ ì²˜ë¦¬\n",
    "degrees_dir = csvs_dir / \"degrees\" if (csvs_dir / \"degrees\").exists() else CURRENT_DIR / \"degrees\"\n",
    "degree_sentences_file = csvs_dir / \"degree_sentences_all.csvì˜ ì‚¬ë³¸\" if (csvs_dir / \"degree_sentences_all.csvì˜ ì‚¬ë³¸\").exists() else degrees_dir / \"degree_sentences_all.csvì˜ ì‚¬ë³¸\"\n",
    "degree_latest_file = csvs_dir / \"degree_latest.csvì˜ ì‚¬ë³¸\" if (csvs_dir / \"degree_latest.csvì˜ ì‚¬ë³¸\").exists() else degrees_dir / \"degree_latest.csvì˜ ì‚¬ë³¸\"\n",
    "\n",
    "# ìš°ì„ ìˆœìœ„ 1: ì´ë¯¸ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜ëœ íŒŒì¼ ì‚¬ìš©\n",
    "if degree_sentences_file.exists():\n",
    "    df_sentences = pd.read_csv(degree_sentences_file, encoding='utf-8-sig')\n",
    "    for _, row in df_sentences.iterrows():\n",
    "        sentence = str(row.get(\"sentence\", \"\")).strip()\n",
    "        if sentence:\n",
    "            csv_documents.append(Document(\n",
    "                page_content=sentence,\n",
    "                metadata={\"source\": \"[ë³„í‘œ 2] í•™ì‚¬í•™ìœ„ì˜ ì¢…ë¥˜\", \"page\": 51, \"type\": \"degree\"}\n",
    "            ))\n",
    "    print(f\"âœ… degree_sentences_all.csv ì²˜ë¦¬ ì™„ë£Œ: {len(df_sentences)}ê°œ ë¬¸ì¥\")\n",
    "# ìš°ì„ ìˆœìœ„ 2: degree_latest.csvë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ ìƒì„±\n",
    "elif degree_latest_file.exists():\n",
    "    df_degrees = pd.read_csv(degree_latest_file, encoding='utf-8-sig')\n",
    "    df_degrees = df_degrees.dropna(subset=['ëŒ€í•™', 'í•™ì‚¬', 'ì „ê³µ'], how='all')\n",
    "    degree_sentences = build_degree_sentences_from_csv(df_degrees, \"ìµœì‹  ê°œì •\")\n",
    "    for sentence in degree_sentences:\n",
    "        csv_documents.append(Document(\n",
    "            page_content=sentence,\n",
    "            metadata={\"source\": \"[ë³„í‘œ 2] í•™ì‚¬í•™ìœ„ì˜ ì¢…ë¥˜\", \"page\": 51, \"type\": \"degree\"}\n",
    "        ))\n",
    "    print(f\"âœ… degree_latest.csv ì²˜ë¦¬ ì™„ë£Œ: {len(degree_sentences)}ê°œ ë¬¸ì¥\")\n",
    "else:\n",
    "    print(\"âš ï¸ degrees ê´€ë ¨ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 2) capacity.csv ì²˜ë¦¬ (ì…í•™ì •ì› ë°ì´í„°)\n",
    "capacity_path = csvs_dir / \"capacity.csv\"\n",
    "if not capacity_path.exists():\n",
    "    capacity_path = CURRENT_DIR / \"capacity.csv\"\n",
    "\n",
    "if capacity_path.exists():\n",
    "    df_capacity = pd.read_csv(capacity_path, encoding='utf-8-sig')\n",
    "\n",
    "    for _, row in df_capacity.iterrows():\n",
    "        text = build_quota_text_v2(row)\n",
    "        if text:\n",
    "            csv_documents.append(Document(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    \"source\": f\"[ì…í•™ì •ì›] {row.get('year')}\",\n",
    "                    \"type\": \"quota\",\n",
    "                    \"year\": row.get(\"year\"),\n",
    "                    \"college\": row.get(\"college\"),\n",
    "                    \"major\": row.get(\"major\")\n",
    "                }\n",
    "            ))\n",
    "    print(\"capacity ì²˜ë¦¬ ì™„ë£Œ:\", len([d for d in csv_documents if d.metadata.get('type')=='quota']))\n",
    "else:\n",
    "    print(\"âš ï¸ capacity.csv íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 3) contract_dept.csv ì²˜ë¦¬\n",
    "contract_path = csvs_dir / \"contract_dept.csv\"\n",
    "if not contract_path.exists():\n",
    "    contract_path = CURRENT_DIR / \"contract_dept.csv\"\n",
    "\n",
    "if contract_path.exists():\n",
    "    df_contract = pd.read_csv(contract_path, encoding='utf-8-sig')\n",
    "    for _, row in df_contract.iterrows():\n",
    "        text = build_contract_text(row)\n",
    "        if text and text.strip():\n",
    "            csv_documents.append(Document(\n",
    "                page_content=text,\n",
    "                metadata={\"source\": \"[ë³„í‘œ 3] ê³„ì•½í•™ê³¼ ì„¤ì¹˜Â·ìš´ì˜\", \"page\": 53, \"type\": \"contract\"}\n",
    "            ))\n",
    "    print(f\"âœ… contract_dept.csv ì²˜ë¦¬ ì™„ë£Œ: {len([d for d in csv_documents if d.metadata.get('type') == 'contract'])}ê°œ ë¬¸ì„œ\")\n",
    "else:\n",
    "    print(\"âš ï¸ contract_dept.csv íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 4) grade.csv ì²˜ë¦¬ (1980ë…„ ê¸°ì¤€ ì„±ì ì  ë°ì´í„°)\n",
    "grade_path = csvs_dir / \"grade.csv\"\n",
    "if not grade_path.exists():\n",
    "    grade_path = CURRENT_DIR / \"grade.csv\"\n",
    "\n",
    "if grade_path.exists():\n",
    "    df_grade = pd.read_csv(grade_path, encoding='utf-8-sig')\n",
    "    for _, row in df_grade.iterrows():\n",
    "        text = build_grade_text(row)\n",
    "        if text and text.strip():\n",
    "            csv_documents.append(Document(\n",
    "                page_content=text,\n",
    "                metadata={\"source\": \"[ë³„í‘œ] ì„±ì ì  ë“±ê¸‰í‘œ\", \"page\": 0, \"type\": \"grade\"}\n",
    "            ))\n",
    "    print(f\"âœ… grade.csv ì²˜ë¦¬ ì™„ë£Œ: {len([d for d in csv_documents if d.metadata.get('type') == 'grade'])}ê°œ ë¬¸ì„œ\")\n",
    "else:\n",
    "    print(\"âš ï¸ grade.csv íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "print(f\"\\nğŸ“¦ ì´ CSV ë¬¸ì„œ: {len(csv_documents)}ê°œ\")\n",
    "\n",
    "# ìµœì¢…ì ìœ¼ë¡œ ì´í™”ì—¬ëŒ€ ë¬¸ì„œ í•©ì¹˜ê¸°\n",
    "ewha_full_docs = ewha_documents + csv_documents\n",
    "print(f\"\\nğŸ“š ì´í™”ì—¬ëŒ€ ë¡œì»¬ DB ì¤€ë¹„ ì™„ë£Œ: ì´ {len(ewha_full_docs)}ê°œ ë¬¸ì„œ\")\n",
    "print(f\"   - PDF í…ìŠ¤íŠ¸: {len(ewha_documents)}ê°œ\")\n",
    "print(f\"   - CSV í‘œ ë°ì´í„°: {len(csv_documents)}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6534bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# 2. ì´í™”ì—¬ëŒ€ ì „ìš© Vector Store ìƒì„± (Local Knowledge Base)\n",
    "# ==================================================================\n",
    "\n",
    "embeddings = UpstageEmbeddings(api_key=UPSTAGE_API_KEY, model=\"solar-embedding-1-large\")\n",
    "\n",
    "# Windows í•œê¸€ ê²½ë¡œ ë¬¸ì œ í•´ê²°: ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš© (RAG_í†µí•©.ipynb ë°©ì‹)\n",
    "# FAISSì˜ C++ ë°±ì—”ë“œê°€ í•œê¸€ ê²½ë¡œë¥¼ ì²˜ë¦¬í•˜ì§€ ëª»í•˜ëŠ” ë¬¸ì œ í•´ê²°\n",
    "import platform\n",
    "import tempfile\n",
    "\n",
    "if platform.system() == 'Windows':\n",
    "    # Windows: í™˜ê²½ ë³€ìˆ˜ TEMP ì‚¬ìš© (ë³´í†µ C:\\Users\\USERNAME\\AppData\\Local\\Temp)\n",
    "    temp_dir = Path(tempfile.gettempdir())\n",
    "    # í”„ë¡œì íŠ¸ë³„ ê³ ìœ í•œ ë””ë ‰í† ë¦¬ëª… ìƒì„±\n",
    "    vector_db_base = temp_dir / \"rag_ewha_vectorstore\"\n",
    "    vector_db_base.mkdir(parents=True, exist_ok=True)\n",
    "    EWHA_VECTOR_DB_PATH_STR = str(vector_db_base.resolve())\n",
    "    EWHA_VECTOR_DB_PATH = vector_db_base\n",
    "    print(f\"ğŸ“‚ Windows í•œê¸€ ê²½ë¡œ ë¬¸ì œ í•´ê²°: ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©\")\n",
    "    print(f\"ğŸ“‚ ë²¡í„° DB ê²½ë¡œ: {EWHA_VECTOR_DB_PATH_STR}\")\n",
    "else:\n",
    "    # Windowsê°€ ì•„ë‹Œ ê²½ìš° ìƒëŒ€ ê²½ë¡œ ì‚¬ìš©\n",
    "    EWHA_VECTOR_DB_PATH = CURRENT_DIR / \"vectorstore_ewha\"\n",
    "    EWHA_VECTOR_DB_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    EWHA_VECTOR_DB_PATH_STR = str(EWHA_VECTOR_DB_PATH.resolve())\n",
    "\n",
    "ewha_vector_store = None\n",
    "\n",
    "vector_db_dir = Path(EWHA_VECTOR_DB_PATH_STR)\n",
    "\n",
    "if vector_db_dir.exists() and (vector_db_dir / \"index.faiss\").exists():\n",
    "    try:\n",
    "        ewha_vector_store = FAISS.load_local(\n",
    "            folder_path=EWHA_VECTOR_DB_PATH_STR,\n",
    "            embeddings=embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        print(\"ğŸ“‚ ê¸°ì¡´ ì´í™”ì—¬ëŒ€ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤. (ì—ëŸ¬: {e})\")\n",
    "        ewha_vector_store = None\n",
    "\n",
    "if ewha_vector_store is None and ewha_full_docs:\n",
    "    # âš ï¸ ì•ˆì „ì¥ì¹˜: ê° ë¬¸ì„œì˜ ê¸¸ì´ í™•ì¸ (4000 í† í° = ì•½ 3000ì ì œí•œ)\n",
    "    MAX_CHUNK_LENGTH = 3000  # ì•ˆì „í•˜ê²Œ 3000ìë¡œ ì„¤ì •\n",
    "    filtered_docs = []\n",
    "    \n",
    "    for doc in ewha_full_docs:\n",
    "        if len(doc.page_content) > MAX_CHUNK_LENGTH:\n",
    "            # ë¬¸ì„œê°€ ë„ˆë¬´ ê¸¸ë©´ ì¶”ê°€ë¡œ ë¶„í• \n",
    "            print(f\"âš ï¸ ë¬¸ì„œê°€ {len(doc.page_content)}ìë¡œ ë„ˆë¬´ ê¹ë‹ˆë‹¤. ì¶”ê°€ ë¶„í•  ì¤‘...\")\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=MAX_CHUNK_LENGTH - 200,  # overlapì„ ìœ„í•´ 200ì ì—¬ìœ \n",
    "                chunk_overlap=100,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \". \", \" \"],\n",
    "                length_function=len\n",
    "            )\n",
    "            sub_chunks = text_splitter.split_text(doc.page_content)\n",
    "            for i, chunk in enumerate(sub_chunks):\n",
    "                if len(chunk) <= MAX_CHUNK_LENGTH:\n",
    "                    new_doc = Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={**doc.metadata, \"sub_chunk_index\": i}\n",
    "                    )\n",
    "                    filtered_docs.append(new_doc)\n",
    "                else:\n",
    "                    print(f\"  âš ï¸ ì¶”ê°€ ë¶„í•  í›„ì—ë„ ì²­í¬ê°€ {len(chunk)}ìì…ë‹ˆë‹¤. ë” ì‘ê²Œ ë¶„í• í•©ë‹ˆë‹¤.\")\n",
    "                    # ì¬ê·€ì ìœ¼ë¡œ ë” ì‘ê²Œ ë¶„í• \n",
    "                    text_splitter_small = RecursiveCharacterTextSplitter(\n",
    "                        chunk_size=1000,\n",
    "                        chunk_overlap=100,\n",
    "                        separators=[\"\\n\\n\", \"\\n\", \". \", \" \"],\n",
    "                        length_function=len\n",
    "                    )\n",
    "                    tiny_chunks = text_splitter_small.split_text(chunk)\n",
    "                    for j, tiny_chunk in enumerate(tiny_chunks):\n",
    "                        new_doc = Document(\n",
    "                            page_content=tiny_chunk,\n",
    "                            metadata={**doc.metadata, \"sub_chunk_index\": i, \"tiny_chunk_index\": j}\n",
    "                        )\n",
    "                        filtered_docs.append(new_doc)\n",
    "        else:\n",
    "            filtered_docs.append(doc)\n",
    "    \n",
    "    print(f\"ğŸ”„ ì´í™”ì—¬ëŒ€ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘... (ì´ {len(filtered_docs)}ê°œ ë¬¸ì„œ)\")\n",
    "    print(f\"   - ìµœëŒ€ ë¬¸ì„œ ê¸¸ì´: {max(len(d.page_content) for d in filtered_docs) if filtered_docs else 0}ì\")\n",
    "    \n",
    "    # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„ë² ë”© (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "    try:\n",
    "        ewha_vector_store = FAISS.from_documents(filtered_docs, embeddings)\n",
    "        # RAG_í†µí•©.ipynb ë°©ì‹: ë¬¸ìì—´ ê²½ë¡œ ì‚¬ìš© (ì„ì‹œ ë””ë ‰í† ë¦¬ëŠ” ì˜ë¬¸ ê²½ë¡œ)\n",
    "        ewha_vector_store.save_local(EWHA_VECTOR_DB_PATH_STR)\n",
    "        print(\"âœ… ìƒì„± ì™„ë£Œ\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "        print(\"   ê¸´ ë¬¸ì„œë¥¼ ì°¾ì•„ì„œ ë” ì‘ê²Œ ë¶„í• í•˜ëŠ” ì¤‘...\")\n",
    "        # ì‹¤íŒ¨ ì‹œ ëª¨ë“  ë¬¸ì„œë¥¼ 1000ìë¡œ ê°•ì œ ë¶„í• \n",
    "        final_docs = []\n",
    "        for doc in filtered_docs:\n",
    "            if len(doc.page_content) > 1000:\n",
    "                text_splitter_final = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=1000,\n",
    "                    chunk_overlap=100,\n",
    "                    separators=[\"\\n\\n\", \"\\n\", \". \", \" \"],\n",
    "                    length_function=len\n",
    "                )\n",
    "                chunks = text_splitter_final.split_text(doc.page_content)\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    final_docs.append(Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={**doc.metadata, \"final_chunk_index\": i}\n",
    "                    ))\n",
    "            else:\n",
    "                final_docs.append(doc)\n",
    "        \n",
    "        print(f\"   ìµœì¢… ë¬¸ì„œ ìˆ˜: {len(final_docs)}ê°œ\")\n",
    "        ewha_vector_store = FAISS.from_documents(final_docs, embeddings)\n",
    "        # RAG_í†µí•©.ipynb ë°©ì‹: ë¬¸ìì—´ ê²½ë¡œ ì‚¬ìš© (ì„ì‹œ ë””ë ‰í† ë¦¬ëŠ” ì˜ë¬¸ ê²½ë¡œ)\n",
    "        ewha_vector_store.save_local(EWHA_VECTOR_DB_PATH_STR)\n",
    "        print(\"âœ… ìƒì„± ì™„ë£Œ (ê°•ì œ ë¶„í•  ì ìš©)\")\n",
    "\n",
    "# ì´í™”ì—¬ëŒ€ ì „ìš© Retriever\n",
    "ewha_retriever = ewha_vector_store.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7c035bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í†µí•© ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ: C:\\Users\\user\\AppData\\Local\\Temp\\rag_ewha_vectorstore_with_wiki\n",
      "\n",
      "âœ… í†µí•© ë²¡í„° ìŠ¤í† ì–´ ì¤€ë¹„ ì™„ë£Œ (Wikipedia í¬í•¨)\n",
      "   - EWHA ë¬¸ì œ: ì´í™”ì—¬ëŒ€ ë²¡í„° ìŠ¤í† ì–´ë§Œ ì‚¬ìš©\n",
      "   - MMLU ë¬¸ì œ: í†µí•© ë²¡í„° ìŠ¤í† ì–´ ì‚¬ìš© (ì´í™”ì—¬ëŒ€ + Wikipedia)\n",
      "âœ… NLTK ê¸°ë°˜ Wikipedia ê²€ìƒ‰ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# 3. ì‚¬ì „ ë²¡í„°í™” Wikipedia ë¬¸ì„œ ì¶”ê°€ (NLTK ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ)\n",
    "# ==================================================================\n",
    "\n",
    "import nltk\n",
    "import time \n",
    "\n",
    "# NLTK ë¦¬ì†ŒìŠ¤ (í•œ ë²ˆë§Œ ë‹¤ìš´ë¡œë“œ, ì´í›„ì—” ìºì‹œ ì‚¬ìš©)\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3-0. ì§ˆë¬¸ì—ì„œ Wikipedia ê²€ìƒ‰ìš© í‚¤ì›Œë“œ ì¶”ì¶œ (NLTK ê¸°ë°˜)\n",
    "# --------------------------------------------------\n",
    "\n",
    "def extract_wiki_terms(question: str, max_terms: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    ì§ˆë¬¸ì—ì„œ Wikipedia ê²€ìƒ‰ìš© í›„ë³´ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•œë‹¤.\n",
    "    - NLTK ê¸°ë°˜ ê³ ìœ ëª…ì‚¬(NNP), ê¸¸ì´ê°€ ê¸´ ëª…ì‚¬ ìš°ì„ \n",
    "    - ë„ë©”ì¸ë³„ ì •êµí™”ëœ í‚¤ì›Œë“œ ë§¤í•‘ ì ìš©\n",
    "    - ë¶ˆí•„ìš”í•œ ë‹¨ì–´ í•„í„°ë§ (QUESTION, QUESTION26 ë“±)\n",
    "    \"\"\"\n",
    "    q = question.lower()\n",
    "\n",
    "    # ë¶ˆí•„ìš”í•œ ë‹¨ì–´ í•„í„°ë§ ë¦¬ìŠ¤íŠ¸\n",
    "    stop_words = {'question', 'questions', 'question26', 'question27', 'question28', 'question29', \n",
    "                  'question30', 'following', 'information', 'refers', 'refers to', 'this', 'that',\n",
    "                  'which', 'what', 'who', 'when', 'where', 'why', 'how', 'the', 'a', 'an',\n",
    "                  'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n",
    "                  'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might',\n",
    "                  'can', 'cannot', 'must', 'shall', 'all', 'some', 'any', 'none', 'both',\n",
    "                  'each', 'every', 'other', 'another', 'such', 'same', 'different', 'various'}\n",
    "\n",
    "    # 1ï¸âƒ£ ë„ë©”ì¸ ê¸°ë°˜ ìš°ì„  ì²˜ë¦¬ ë§µ (ì •ë°€ ë§¤í•‘)\n",
    "    topic_map = [\n",
    "        # ì² í•™\n",
    "        ([\"kohlberg\"], [\"Kohlberg's stages of moral development\"]),\n",
    "        ([\"moral development\"], [\"Kohlberg's stages of moral development\"]),\n",
    "        ([\"kant\", \"imperative\"], [\"Categorical imperative\", \"Immanuel Kant\"]),\n",
    "        ([\"utilitarian\", \"pleasure\", \"pain\"], [\"Utilitarianism\", \"Jeremy Bentham\"]),\n",
    "        ([\"positivism\"], [\"Legal positivism\"]),\n",
    "\n",
    "        # í˜•ë²• / ë²”ì£„\n",
    "        ([\"robbery\"], [\"Robbery\"]),\n",
    "        ([\"larceny\"], [\"Larceny\"]),\n",
    "        ([\"burglary\"], [\"Burglary\"]),\n",
    "        ([\"stolen\", \"receiving\"], [\"Receiving stolen property\"]),\n",
    "\n",
    "        # Singer ì˜ì—­\n",
    "        ([\"singer\", \"affluence\", \"famine\"], [\"Famine, Affluence and Morality\", \"Peter Singer\"]),\n",
    "    ]\n",
    "\n",
    "    # 1ï¸âƒ£ ë„ë©”ì¸ ê¸°ë°˜ ë§µí•‘ ê·¸ëŒ€ë¡œ\n",
    "    for triggers, wiki_titles in topic_map:\n",
    "        if any(t in q for t in triggers):\n",
    "            return wiki_titles[:max_terms]\n",
    "\n",
    "    # 2ï¸âƒ£ NLTK noun ê¸°ë°˜ fallback\n",
    "    try:\n",
    "        tokens = nltk.word_tokenize(question)\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "    except Exception:\n",
    "        tokens = re.findall(r\"[A-Za-z][A-Za-z\\-]{2,}\", question)\n",
    "        filtered_tokens = [t for t in tokens if t.lower() not in stop_words]\n",
    "        return filtered_tokens[:max_terms]\n",
    "\n",
    "    candidates = [\n",
    "        word for word, pos in tagged\n",
    "        if pos.startswith(\"NNP\") or (pos.startswith(\"NN\") and len(word) >= 6)\n",
    "    ]\n",
    "\n",
    "    cleaned = []\n",
    "    for w in candidates:\n",
    "        w = re.sub(r\"[^A-Za-z\\-]\", \"\", w)\n",
    "        if len(w) >= 3 and w.lower() not in stop_words:\n",
    "            cleaned.append(w)\n",
    "\n",
    "    unique = []\n",
    "    seen = set()\n",
    "    for w in cleaned:\n",
    "        wl = w.lower()\n",
    "        if wl not in seen:\n",
    "            seen.add(wl)\n",
    "            unique.append(w)\n",
    "\n",
    "    return unique[:max_terms]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3-1. Wikipedia ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜\n",
    "# --------------------------------------------------\n",
    "\n",
    "# ì‹¤íŒ¨í•œ í˜ì´ì§€ì— ëŒ€í•œ ëŒ€ì²´ ì œëª© ë§¤í•‘\n",
    "TITLE_ALTERNATIVES = {\n",
    "    \"Emergency ethics\": [\"Medical ethics\", \"Emergency medical ethics\", \"Bioethics\", \"Clinical ethics\"],\n",
    "    \"Arctic archaeology\": [\"Archaeology of the Arctic\", \"Arctic Archaeology\", \"Prehistory of Alaska\", \"Alaska archaeology\"],\n",
    "    \"Denali complex\": [\"Denali National Park\", \"Denali\", \"Mount Denali\", \"Alaska Range\"],\n",
    "    \"Nenana complex\": [\"Nenana Valley\", \"Nenana River\", \"Alaska archaeology\", \"Prehistory of Alaska\"]\n",
    "}\n",
    "\n",
    "def generate_title_variations(title: str) -> List[str]:\n",
    "    \"\"\"ì œëª©ì˜ ë‹¤ì–‘í•œ ë³€í˜• ìƒì„±\"\"\"\n",
    "    variations = [title]  # ì›ë³¸ ì œëª©\n",
    "    \n",
    "    # ëŒ€ì²´ ì œëª© ë§¤í•‘ í™•ì¸\n",
    "    if title in TITLE_ALTERNATIVES:\n",
    "        variations.extend(TITLE_ALTERNATIVES[title])\n",
    "    \n",
    "    # ì œëª© ë³€í˜• ìƒì„±\n",
    "    # 1. ì²« ê¸€ì ëŒ€ë¬¸ì ë³€í˜•\n",
    "    if title != title.capitalize():\n",
    "        variations.append(title.capitalize())\n",
    "    \n",
    "    # 2. ë‹¨ì–´ë³„ ì²« ê¸€ì ëŒ€ë¬¸ì\n",
    "    title_title = title.title()\n",
    "    if title_title != title:\n",
    "        variations.append(title_title)\n",
    "    \n",
    "    # 3. ë‹¨ì–´ ìˆœì„œ ë³€ê²½ (2ë‹¨ì–´ì¸ ê²½ìš°)\n",
    "    words = title.split()\n",
    "    if len(words) == 2:\n",
    "        variations.append(f\"{words[1]} {words[0]}\")\n",
    "    \n",
    "    # ì¤‘ë³µ ì œê±° ë° ìˆœì„œ ìœ ì§€\n",
    "    seen = set()\n",
    "    unique_variations = []\n",
    "    for var in variations:\n",
    "        if var not in seen:\n",
    "            seen.add(var)\n",
    "            unique_variations.append(var)\n",
    "    \n",
    "    return unique_variations\n",
    "\n",
    "def fetch_wikipedia_article(title: str, language: str = \"en\") -> Optional[str]:\n",
    "    \n",
    "    \n",
    "    max_retries=3\n",
    "    retry_delay=2\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # wikipediaapi ì‚¬ìš©\n",
    "            wiki_wiki = wikipediaapi.Wikipedia(\n",
    "                user_agent='MyNLPProject/1.0',\n",
    "                language=language,\n",
    "                extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    "            )\n",
    "                    \n",
    "            # ì œëª© ë³€í˜• ëª©ë¡ ìƒì„±\n",
    "            title_variations = generate_title_variations(title)\n",
    "            \n",
    "            # ê° ì œëª© ë³€í˜• ì‹œë„\n",
    "            for attempt_title in title_variations:\n",
    "                page = wiki_wiki.page(attempt_title)\n",
    "                \n",
    "                if page.exists():\n",
    "                    # ë™ìŒì´ì˜ì–´ í˜ì´ì§€ í™•ì¸\n",
    "                    text_lower = page.text.lower()\n",
    "                    if \"may refer to\" in text_lower or \"disambiguation\" in text_lower:\n",
    "                        # ë™ìŒì´ì˜ì–´ í˜ì´ì§€ì¸ ê²½ìš°, ì²« ë²ˆì§¸ ë§í¬ ì‹œë„\n",
    "                        if page.links:\n",
    "                            first_link_title = list(page.links.keys())[0]\n",
    "                            if attempt_title != title:\n",
    "                                print(f\"   ğŸ”„ ëŒ€ì²´ ì œëª© ì„±ê³µ: '{title}' â†’ '{attempt_title}' (ë™ìŒì´ì˜ì–´ â†’ '{first_link_title}')\")\n",
    "                            else:\n",
    "                                print(f\"   ğŸ”„ ë™ìŒì´ì˜ì–´ í˜ì´ì§€ ê°ì§€: '{title}' â†’ '{first_link_title}' ì‹œë„\")\n",
    "                            disambiguation_page = wiki_wiki.page(first_link_title)\n",
    "                            if disambiguation_page.exists():\n",
    "                                return disambiguation_page.text\n",
    "                        # ë§í¬ê°€ ì—†ìœ¼ë©´ ì›ë³¸ í˜ì´ì§€ í…ìŠ¤íŠ¸ ë°˜í™˜\n",
    "                        if attempt_title != title:\n",
    "                            print(f\"   ğŸ”„ ëŒ€ì²´ ì œëª© ì„±ê³µ: '{title}' â†’ '{attempt_title}'\")\n",
    "                        return page.text\n",
    "                    else:\n",
    "                        # ì¼ë°˜ í˜ì´ì§€\n",
    "                        if attempt_title != title:\n",
    "                            print(f\"   ğŸ”„ ëŒ€ì²´ ì œëª© ì„±ê³µ: '{title}' â†’ '{attempt_title}'\")\n",
    "                        return page.text\n",
    "            \n",
    "                    # ëª¨ë“  ë³€í˜• ì‹¤íŒ¨\n",
    "            print(f\"âš ï¸ Wikipedia ë¬¸ì„œ '{title}' ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (ëª¨ë“  ë³€í˜• ì‹œë„ ì‹¤íŒ¨)\")\n",
    "            return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"   âš ï¸ ì‹œë„ {attempt + 1}/{max_retries} ì‹¤íŒ¨: {type(e).__name__}, {retry_delay}ì´ˆ í›„ ì¬ì‹œë„...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(f\"   âŒ ìµœì¢… ì‹¤íŒ¨: {type(e).__name__}: {str(e)[:100]}\")\n",
    "                return None\n",
    "def create_wikipedia_document(title: str, content: str, priority: str = \"high\") -> List[Document]:\n",
    "    \"\"\"Wikipedia ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• \"\"\"\n",
    "    if not content:\n",
    "        return []\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=120,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_text(content)\n",
    "    documents = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        documents.append(Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\n",
    "                \"source\": f\"Wikipedia: {title}\",\n",
    "                \"type\": \"wikipedia\",\n",
    "                \"priority\": priority,\n",
    "                \"chunk_index\": i\n",
    "            }\n",
    "        ))\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3-2. ì‚¬ì „ ì •ì˜ëœ Wikipedia ë¬¸ì„œ ëª©ë¡\n",
    "# --------------------------------------------------\n",
    "\n",
    "# ìµœìš°ì„  Wikipedia ë¬¸ì„œ ëª©ë¡ (Phase 1)\n",
    "priority_1_wiki_titles = {\n",
    "    \"Kohlberg's stages of moral development\": \"high\",\n",
    "    \"Robbery\": \"high\",\n",
    "    \"Larceny\": \"high\",\n",
    "    \"Burglary\": \"high\",\n",
    "    \"Homo erectus\": \"high\",\n",
    "    \"Neoteny\": \"high\",\n",
    "    # ì˜¤ë‹µ ë¶„ì„ ê¸°ë°˜ ì¶”ê°€ ë¬¸ì„œ\n",
    "    \"Alan Dershowitz\": \"high\",\n",
    "    \"Emergency ethics\": \"high\",\n",
    "    \"Thomas Aquinas\": \"high\",\n",
    "    \"Natural law\": \"high\",\n",
    "    \"Unnatural acts\": \"high\",\n",
    "    \"Negligence\": \"high\",\n",
    "    \"Vicarious liability\": \"high\",\n",
    "    \"Tort law\": \"high\",\n",
    "    \"Assault (tort)\": \"high\",\n",
    "    \"Battery (tort)\": \"high\",\n",
    "    \"Paternity test\": \"high\",\n",
    "    \"Evidence (law)\": \"high\",\n",
    "    \"Strict liability\": \"high\",\n",
    "    \"Product liability\": \"high\",\n",
    "}\n",
    "\n",
    "# ì°¨ìˆœìœ„ Wikipedia ë¬¸ì„œ ëª©ë¡ (Phase 2)\n",
    "priority_2_wiki_titles = {\n",
    "    \"Tang dynasty\": \"medium\",\n",
    "    \"Deutschland Ã¼ber alles\": \"medium\",\n",
    "    \"Bipolar disorder\": \"medium\",\n",
    "    \"Ethics in psychology\": \"medium\",\n",
    "    \"Utilitarianism\": \"medium\",\n",
    "    \"Categorical imperative\": \"medium\",\n",
    "    \"Jurisprudence\": \"medium\",\n",
    "    \"Famine, Affluence and Morality\": \"medium\",\n",
    "}\n",
    "\n",
    "# ë³´ì¡° Wikipedia ë¬¸ì„œ ëª©ë¡ (Phase 3)\n",
    "priority_3_wiki_titles = {\n",
    "    \"Human evolution\": \"low\",\n",
    "    \"Homo habilis\": \"low\",\n",
    "    \"Australopithecus afarensis\": \"low\",\n",
    "    \"Aristotelianism\": \"low\",\n",
    "    \"Peter Singer\": \"low\",\n",
    "    \"Green marketing\": \"low\",\n",
    "    \"Theory of multiple intelligences\": \"low\",\n",
    "    \"Approach-avoidance conflict\": \"low\",\n",
    "    \"Receiving stolen property\": \"low\",\n",
    "    \"Legal positivism\": \"low\",\n",
    "    \"John Frere\": \"low\",\n",
    "    \"Du Fu\": \"low\",\n",
    "    \"Enforcement of foreign judgments\": \"low\",\n",
    "    # ì˜¤ë‹µ ë¶„ì„ ê¸°ë°˜ ì¶”ê°€ ë¬¸ì„œ\n",
    "    \"Lithic technology\": \"low\",\n",
    "    \"Microblade technology\": \"low\",\n",
    "    \"Arctic archaeology\": \"low\",\n",
    "    \"Denali complex\": \"low\",\n",
    "    \"Nenana complex\": \"low\",\n",
    "    \"Divorce\": \"low\",\n",
    "    \"Child custody\": \"low\",\n",
    "    \"Family therapy\": \"low\",\n",
    "}\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3-3. Wikipedia ë¬¸ì„œ ë²¡í„°í™” ë° í†µí•©\n",
    "# --------------------------------------------------\n",
    "\n",
    "# ë²¡í„°í™”í•  ë¬¸ì„œ ì„ íƒ (ê¸°ë³¸ê°’: ì „ì²´ ì ìš©)\n",
    "WIKI_PHASE = 3  # 1: ìµœìš°ì„ , 2: ìµœìš°ì„ +ì°¨ìˆœìœ„, 3: ì „ì²´\n",
    "\n",
    "wiki_documents = []\n",
    "\n",
    "if WIKI_PHASE >= 1:\n",
    "    print(\"\\nğŸ“– Phase 1: ìµœìš°ì„  Wikipedia ë¬¸ì„œ ë²¡í„°í™” ì¤‘...\")\n",
    "    for title, priority in priority_1_wiki_titles.items():\n",
    "        print(f\"   - {title} ë‹¤ìš´ë¡œë“œ ì¤‘...\")\n",
    "        content = fetch_wikipedia_article(title)\n",
    "        if content:\n",
    "            docs = create_wikipedia_document(title, content, priority)\n",
    "            wiki_documents.extend(docs)\n",
    "            print(f\"     âœ… {len(docs)}ê°œ ì²­í¬ ìƒì„±\")\n",
    "        else:\n",
    "            print(f\"     âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨\")\n",
    "\n",
    "if WIKI_PHASE >= 2:\n",
    "    print(\"\\nğŸ“– Phase 2: ì°¨ìˆœìœ„ Wikipedia ë¬¸ì„œ ë²¡í„°í™” ì¤‘...\")\n",
    "    for title, priority in priority_2_wiki_titles.items():\n",
    "        print(f\"   - {title} ë‹¤ìš´ë¡œë“œ ì¤‘...\")\n",
    "        content = fetch_wikipedia_article(title)\n",
    "        if content:\n",
    "            docs = create_wikipedia_document(title, content, priority)\n",
    "            wiki_documents.extend(docs)\n",
    "            print(f\"     âœ… {len(docs)}ê°œ ì²­í¬ ìƒì„±\")\n",
    "        else:\n",
    "            print(f\"     âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨\")\n",
    "\n",
    "if WIKI_PHASE >= 3:\n",
    "    print(\"\\nğŸ“– Phase 3: ë³´ì¡° Wikipedia ë¬¸ì„œ ë²¡í„°í™” ì¤‘...\")\n",
    "    for title, priority in priority_3_wiki_titles.items():\n",
    "        print(f\"   - {title} ë‹¤ìš´ë¡œë“œ ì¤‘...\")\n",
    "        content = fetch_wikipedia_article(title)\n",
    "        if content:\n",
    "            docs = create_wikipedia_document(title, content, priority)\n",
    "            wiki_documents.extend(docs)\n",
    "            print(f\"     âœ… {len(docs)}ê°œ ì²­í¬ ìƒì„±\")\n",
    "        else:\n",
    "            print(f\"     âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3-4. í†µí•© ë²¡í„° ìŠ¤í† ì–´ ìƒì„± (ì´í™”ì—¬ëŒ€ + Wikipedia)\n",
    "# --------------------------------------------------\n",
    "\n",
    "# ëª¨ë“  ë¬¸ì„œ í†µí•©\n",
    "all_documents_for_vectorstore = []\n",
    "\n",
    "# ì´í™”ì—¬ëŒ€ ë¬¸ì„œ ì¶”ê°€\n",
    "if ewha_full_docs:\n",
    "    all_documents_for_vectorstore.extend(ewha_full_docs)\n",
    "    print(f\"\\nğŸ“š ì´í™”ì—¬ëŒ€ ë¬¸ì„œ: {len(ewha_full_docs)}ê°œ ì²­í¬\")\n",
    "\n",
    "# Wikipedia ë¬¸ì„œ ì¶”ê°€\n",
    "if wiki_documents:\n",
    "    all_documents_for_vectorstore.extend(wiki_documents)\n",
    "    print(f\"ğŸ“š Wikipedia ë¬¸ì„œ: {len(wiki_documents)}ê°œ ì²­í¬ ì¶”ê°€\")\n",
    "\n",
    "print(f\"\\nğŸ“š ì´ í†µí•© ë¬¸ì„œ: {len(all_documents_for_vectorstore)}ê°œ\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3-6. í†µí•© ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ë˜ëŠ” ë¡œë“œ\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Windows í•œê¸€ ê²½ë¡œ ë¬¸ì œ í•´ê²°\n",
    "import platform\n",
    "import tempfile\n",
    "\n",
    "if platform.system() == 'Windows':\n",
    "    temp_dir = Path(tempfile.gettempdir())\n",
    "    vector_db_base = temp_dir / \"rag_ewha_vectorstore_with_wiki\"\n",
    "    vector_db_base.mkdir(parents=True, exist_ok=True)\n",
    "    INTEGRATED_VECTOR_DB_PATH_STR = str(vector_db_base.resolve())\n",
    "    INTEGRATED_VECTOR_DB_PATH = vector_db_base\n",
    "    print(f\"ğŸ“‚ í†µí•© ë²¡í„° DB ê²½ë¡œ: {INTEGRATED_VECTOR_DB_PATH_STR}\")\n",
    "else:\n",
    "    INTEGRATED_VECTOR_DB_PATH = CURRENT_DIR / \"vectorstore_integrated\"\n",
    "    INTEGRATED_VECTOR_DB_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    INTEGRATED_VECTOR_DB_PATH_STR = str(INTEGRATED_VECTOR_DB_PATH.resolve())\n",
    "\n",
    "integrated_vector_store = None\n",
    "vector_db_dir = Path(INTEGRATED_VECTOR_DB_PATH_STR)\n",
    "\n",
    "# í†µí•© ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹œë„\n",
    "FORCE_REBUILD = True  # Trueë¡œ ì„¤ì •í•˜ë©´ ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¬´ì‹œí•˜ê³  ì¬ìƒì„± (ìƒˆë¡œ ì¶”ê°€í•œ Wikipedia ë¬¸ì„œ ë°˜ì˜)\n",
    "\n",
    "if not FORCE_REBUILD and vector_db_dir.exists() and (vector_db_dir / \"index.faiss\").exists():\n",
    "    try:\n",
    "        integrated_vector_store = FAISS.load_local(\n",
    "            folder_path=INTEGRATED_VECTOR_DB_PATH_STR,\n",
    "            embeddings=embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        print(\"ğŸ“‚ ê¸°ì¡´ í†µí•© ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ (Wikipedia í¬í•¨)\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤. (ì—ëŸ¬: {e})\")\n",
    "        integrated_vector_store = None\n",
    "\n",
    "# ë²¡í„° ìŠ¤í† ì–´ê°€ ì—†ê±°ë‚˜ ì¬ìƒì„±í•´ì•¼ í•˜ëŠ” ê²½ìš°\n",
    "if integrated_vector_store is None or FORCE_REBUILD:\n",
    "    if all_documents_for_vectorstore:\n",
    "        print(f\"\\nâ³ í†µí•© ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘... (ì´ {len(all_documents_for_vectorstore)}ê°œ ë¬¸ì„œ)\")\n",
    "        \n",
    "        # ë¬¸ì„œ ê¸¸ì´ ì²´í¬ ë° í•„í„°ë§\n",
    "        MAX_CHUNK_LENGTH = 3000\n",
    "        filtered_docs = []\n",
    "        \n",
    "        for doc in all_documents_for_vectorstore:\n",
    "            if len(doc.page_content) > MAX_CHUNK_LENGTH:\n",
    "                print(f\"âš ï¸ ë¬¸ì„œê°€ {len(doc.page_content)}ìë¡œ ë„ˆë¬´ ê¹ë‹ˆë‹¤. ì¶”ê°€ ë¶„í•  ì¤‘...\")\n",
    "                text_splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=MAX_CHUNK_LENGTH - 200,\n",
    "                    chunk_overlap=100,\n",
    "                    separators=[\"\\n\\n\", \"\\n\", \". \", \" \"],\n",
    "                    length_function=len\n",
    "                )\n",
    "                sub_chunks = text_splitter.split_text(doc.page_content)\n",
    "                for i, chunk in enumerate(sub_chunks):\n",
    "                    if len(chunk) <= MAX_CHUNK_LENGTH:\n",
    "                        new_doc = Document(\n",
    "                            page_content=chunk,\n",
    "                            metadata={**doc.metadata, \"sub_chunk_index\": i}\n",
    "                        )\n",
    "                        filtered_docs.append(new_doc)\n",
    "                    else:\n",
    "                        # ì¬ê·€ì ìœ¼ë¡œ ë” ì‘ê²Œ ë¶„í• \n",
    "                        text_splitter_small = RecursiveCharacterTextSplitter(\n",
    "                            chunk_size=1000,\n",
    "                            chunk_overlap=100,\n",
    "                            separators=[\"\\n\\n\", \"\\n\", \". \", \" \"],\n",
    "                            length_function=len\n",
    "                        )\n",
    "                        tiny_chunks = text_splitter_small.split_text(chunk)\n",
    "                        for j, tiny_chunk in enumerate(tiny_chunks):\n",
    "                            new_doc = Document(\n",
    "                                page_content=tiny_chunk,\n",
    "                                metadata={**doc.metadata, \"sub_chunk_index\": i, \"tiny_chunk_index\": j}\n",
    "                            )\n",
    "                            filtered_docs.append(new_doc)\n",
    "            else:\n",
    "                filtered_docs.append(doc)\n",
    "        \n",
    "        print(f\"   ìµœì¢… ë¬¸ì„œ ìˆ˜: {len(filtered_docs)}ê°œ\")\n",
    "        \n",
    "        try:\n",
    "            integrated_vector_store = FAISS.from_documents(filtered_docs, embeddings)\n",
    "            integrated_vector_store.save_local(INTEGRATED_VECTOR_DB_PATH_STR)\n",
    "            print(f\"âœ… í†µí•© ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ: {INTEGRATED_VECTOR_DB_PATH_STR}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "            print(\"   ë” ì‘ì€ ì²­í¬ë¡œ ì¬ì‹œë„ ì¤‘...\")\n",
    "            # ì‹¤íŒ¨ ì‹œ ê°•ì œ ë¶„í• \n",
    "            final_docs = []\n",
    "            for doc in filtered_docs:\n",
    "                if len(doc.page_content) > 1000:\n",
    "                    text_splitter_final = RecursiveCharacterTextSplitter(\n",
    "                        chunk_size=1000,\n",
    "                        chunk_overlap=100,\n",
    "                        separators=[\"\\n\\n\", \"\\n\", \". \", \" \"],\n",
    "                        length_function=len\n",
    "                    )\n",
    "                    chunks = text_splitter_final.split_text(doc.page_content)\n",
    "                    for i, chunk in enumerate(chunks):\n",
    "                        final_docs.append(Document(\n",
    "                            page_content=chunk,\n",
    "                            metadata={**doc.metadata, \"final_chunk_index\": i}\n",
    "                        ))\n",
    "                else:\n",
    "                    final_docs.append(doc)\n",
    "            \n",
    "            print(f\"   ìµœì¢… ë¬¸ì„œ ìˆ˜: {len(final_docs)}ê°œ\")\n",
    "            integrated_vector_store = FAISS.from_documents(final_docs, embeddings)\n",
    "            integrated_vector_store.save_local(INTEGRATED_VECTOR_DB_PATH_STR)\n",
    "            print(\"âœ… ìƒì„± ì™„ë£Œ (ê°•ì œ ë¶„í•  ì ìš©)\")\n",
    "    else:\n",
    "        print(\"âš ï¸ í†µí•©í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# í†µí•© ë²¡í„° ìŠ¤í† ì–´ (MMLU ë¬¸ì œìš© - Wikipedia í¬í•¨)\n",
    "# EWHA ë¬¸ì œëŠ” ê¸°ì¡´ ewha_vector_storeë§Œ ì‚¬ìš© (ì´í™”ì—¬ëŒ€ ë¬¸ì„œë§Œ)\n",
    "# MMLU ë¬¸ì œëŠ” integrated_vector_store ì‚¬ìš© (Wikipedia ë¬¸ì„œ í¬í•¨)\n",
    "\n",
    "if integrated_vector_store is not None:\n",
    "    print(\"\\nâœ… í†µí•© ë²¡í„° ìŠ¤í† ì–´ ì¤€ë¹„ ì™„ë£Œ (Wikipedia í¬í•¨)\")\n",
    "    print(\"   - EWHA ë¬¸ì œ: ì´í™”ì—¬ëŒ€ ë²¡í„° ìŠ¤í† ì–´ë§Œ ì‚¬ìš©\")\n",
    "    print(\"   - MMLU ë¬¸ì œ: í†µí•© ë²¡í„° ìŠ¤í† ì–´ ì‚¬ìš© (ì´í™”ì—¬ëŒ€ + Wikipedia)\")\n",
    "    # MMLUìš© retriever ìƒì„± (í†µí•© ë²¡í„° ìŠ¤í† ì–´ ì‚¬ìš©)\n",
    "    mmlu_retriever = integrated_vector_store.as_retriever(search_kwargs={\"k\": 4})\n",
    "else:\n",
    "    print(\"\\nâš ï¸ í†µí•© ë²¡í„° ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    mmlu_retriever = None\n",
    "\n",
    "# ewha_retrieverëŠ” ê¸°ì¡´ ì´í™”ì—¬ëŒ€ ë²¡í„° ìŠ¤í† ì–´ë§Œ ì‚¬ìš© (ë³€ê²½ ì—†ìŒ)\n",
    "print(\"âœ… NLTK ê¸°ë°˜ Wikipedia ê²€ìƒ‰ ì¤€ë¹„ ì™„ë£Œ\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
